- name: lora_alpaca-rank128-2e-4
  sku: 1xG4
  process_count_per_node: 1
  submit_args:
    container_args:
      cpus: 32
  command:
    - CUDA_VISIBLE_DEVICES=0 torchrun --rdzv-backend=c10d --rdzv-endpoint=localhost:10 --nnodes=1 --nproc-per-node=1 qlora_lora.py --ddp_find_unused_parameters False --model_name_or_path meta-llama/Llama-2-7b-hf --use_auth --output_dir /mnt/t-qingru/exp_results/lora_alpaca/rank128/2e-4/seed0 --logging_steps 10  --save_strategy steps --data_seed 42 --save_steps 2500 --save_total_limit 40 --evaluation_strategy steps --eval_dataset_size 1024  --max_eval_samples 1000 --per_device_eval_batch_size 1 --max_new_tokens 32 --dataloader_num_workers 1 --group_by_length --logging_strategy steps --remove_unused_columns False --do_train --do_eval --do_mmlu_eval --lora_r 128 --lora_alpha 16 --lora_modules all --double_quant --quant_type nf4 --bf16 --bits 16 --warmup_ratio 0.03 --lr_scheduler_type constant --gradient_checkpointing --dataset alpaca --source_max_len 384 --target_max_len 128  --per_device_train_batch_size 1 --gradient_accumulation_steps 16 --max_steps 5000 --eval_steps 2500 --learning_rate 2e-4 --adam_beta2 0.999  --max_grad_norm 0.3 --lora_dropout 0.1 --weight_decay 0.0  --seed 0 --cache_dir /mnt/t-qingru/cache/lora --mmlu_split test &
    - CUDA_VISIBLE_DEVICES=1 torchrun --rdzv-backend=c10d --rdzv-endpoint=localhost:20 --nnodes=1 --nproc-per-node=1 qlora_lora.py --ddp_find_unused_parameters False --model_name_or_path meta-llama/Llama-2-7b-hf --use_auth --output_dir /mnt/t-qingru/exp_results/lora_alpaca/rank128/2e-4/seed1 --logging_steps 10  --save_strategy steps --data_seed 43 --save_steps 2500 --save_total_limit 40 --evaluation_strategy steps --eval_dataset_size 1024  --max_eval_samples 1000 --per_device_eval_batch_size 1 --max_new_tokens 32 --dataloader_num_workers 1 --group_by_length --logging_strategy steps --remove_unused_columns False --do_train --do_eval --do_mmlu_eval --lora_r 128 --lora_alpha 16 --lora_modules all --double_quant --quant_type nf4 --bf16 --bits 16 --warmup_ratio 0.03 --lr_scheduler_type constant --gradient_checkpointing --dataset alpaca --source_max_len 384 --target_max_len 128  --per_device_train_batch_size 1 --gradient_accumulation_steps 16 --max_steps 5000 --eval_steps 2500 --learning_rate 2e-4 --adam_beta2 0.999  --max_grad_norm 0.3 --lora_dropout 0.1 --weight_decay 0.0  --seed 1 --cache_dir /mnt/t-qingru/cache/lora --mmlu_split test &
    - CUDA_VISIBLE_DEVICES=2 torchrun --rdzv-backend=c10d --rdzv-endpoint=localhost:30 --nnodes=1 --nproc-per-node=1 qlora_lora.py --ddp_find_unused_parameters False --model_name_or_path meta-llama/Llama-2-7b-hf --use_auth --output_dir /mnt/t-qingru/exp_results/lora_alpaca/rank128/2e-4/seed2 --logging_steps 10  --save_strategy steps --data_seed 44 --save_steps 2500 --save_total_limit 40 --evaluation_strategy steps --eval_dataset_size 1024  --max_eval_samples 1000 --per_device_eval_batch_size 1 --max_new_tokens 32 --dataloader_num_workers 1 --group_by_length --logging_strategy steps --remove_unused_columns False --do_train --do_eval --do_mmlu_eval --lora_r 128 --lora_alpha 16 --lora_modules all --double_quant --quant_type nf4 --bf16 --bits 16 --warmup_ratio 0.03 --lr_scheduler_type constant --gradient_checkpointing --dataset alpaca --source_max_len 384 --target_max_len 128  --per_device_train_batch_size 1 --gradient_accumulation_steps 16 --max_steps 5000 --eval_steps 2500 --learning_rate 0.0002 --adam_beta2 0.999  --max_grad_norm 0.3 --lora_dropout 0.1 --weight_decay 0.0  --seed 2 --cache_dir /mnt/t-qingru/cache/lora --mmlu_split test &
    - sleep 3600
    - CUDA_VISIBLE_DEVICES=3 torchrun --rdzv-backend=c10d --rdzv-endpoint=localhost:40 --nnodes=1 --nproc-per-node=1 qlora_lora.py --ddp_find_unused_parameters False --model_name_or_path meta-llama/Llama-2-7b-hf --use_auth --output_dir /mnt/t-qingru/exp_results/lora_alpaca/rank128/2e-4/seed3 --logging_steps 10  --save_strategy steps --data_seed 45 --save_steps 2500 --save_total_limit 40 --evaluation_strategy steps --eval_dataset_size 1024  --max_eval_samples 1000 --per_device_eval_batch_size 1 --max_new_tokens 32 --dataloader_num_workers 1 --group_by_length --logging_strategy steps --remove_unused_columns False --do_train --do_eval --do_mmlu_eval --lora_r 128 --lora_alpha 16 --lora_modules all --double_quant --quant_type nf4 --bf16 --bits 16 --warmup_ratio 0.03 --lr_scheduler_type constant --gradient_checkpointing --dataset alpaca --source_max_len 384 --target_max_len 128  --per_device_train_batch_size 1 --gradient_accumulation_steps 16 --max_steps 5000 --eval_steps 2500 --learning_rate 0.0002 --adam_beta2 0.999  --max_grad_norm 0.3 --lora_dropout 0.1 --weight_decay 0.0  --seed 3 --cache_dir /mnt/t-qingru/cache/lora --mmlu_split test

- name: lora_alpaca-rank128-5e-5
  sku: 1xG4
  process_count_per_node: 1
  submit_args:
    container_args:
      cpus: 32
  command:
    - CUDA_VISIBLE_DEVICES=0 torchrun --rdzv-backend=c10d --rdzv-endpoint=localhost:10 --nnodes=1 --nproc-per-node=1 qlora_lora.py --ddp_find_unused_parameters False --model_name_or_path meta-llama/Llama-2-7b-hf --use_auth --output_dir /mnt/t-qingru/exp_results/lora_alpaca/rank128/5e-5/seed0 --logging_steps 10  --save_strategy steps --data_seed 42 --save_steps 2500 --save_total_limit 40 --evaluation_strategy steps --eval_dataset_size 1024  --max_eval_samples 1000 --per_device_eval_batch_size 1 --max_new_tokens 32 --dataloader_num_workers 1 --group_by_length --logging_strategy steps --remove_unused_columns False --do_train --do_eval --do_mmlu_eval --lora_r 128 --lora_alpha 16 --lora_modules all --double_quant --quant_type nf4 --bf16 --bits 16 --warmup_ratio 0.03 --lr_scheduler_type constant --gradient_checkpointing --dataset alpaca --source_max_len 384 --target_max_len 128  --per_device_train_batch_size 1 --gradient_accumulation_steps 16 --max_steps 5000 --eval_steps 2500 --learning_rate 5e-5 --adam_beta2 0.999  --max_grad_norm 0.3 --lora_dropout 0.1 --weight_decay 0.0  --seed 0 --cache_dir /mnt/t-qingru/cache/lora --mmlu_split test &
    - CUDA_VISIBLE_DEVICES=1 torchrun --rdzv-backend=c10d --rdzv-endpoint=localhost:20 --nnodes=1 --nproc-per-node=1 qlora_lora.py --ddp_find_unused_parameters False --model_name_or_path meta-llama/Llama-2-7b-hf --use_auth --output_dir /mnt/t-qingru/exp_results/lora_alpaca/rank128/5e-5/seed1 --logging_steps 10  --save_strategy steps --data_seed 43 --save_steps 2500 --save_total_limit 40 --evaluation_strategy steps --eval_dataset_size 1024  --max_eval_samples 1000 --per_device_eval_batch_size 1 --max_new_tokens 32 --dataloader_num_workers 1 --group_by_length --logging_strategy steps --remove_unused_columns False --do_train --do_eval --do_mmlu_eval --lora_r 128 --lora_alpha 16 --lora_modules all --double_quant --quant_type nf4 --bf16 --bits 16 --warmup_ratio 0.03 --lr_scheduler_type constant --gradient_checkpointing --dataset alpaca --source_max_len 384 --target_max_len 128  --per_device_train_batch_size 1 --gradient_accumulation_steps 16 --max_steps 5000 --eval_steps 2500 --learning_rate 5e-5 --adam_beta2 0.999  --max_grad_norm 0.3 --lora_dropout 0.1 --weight_decay 0.0  --seed 1 --cache_dir /mnt/t-qingru/cache/lora --mmlu_split test &
    - CUDA_VISIBLE_DEVICES=2 torchrun --rdzv-backend=c10d --rdzv-endpoint=localhost:30 --nnodes=1 --nproc-per-node=1 qlora_lora.py --ddp_find_unused_parameters False --model_name_or_path meta-llama/Llama-2-7b-hf --use_auth --output_dir /mnt/t-qingru/exp_results/lora_alpaca/rank128/5e-5/seed2 --logging_steps 10  --save_strategy steps --data_seed 44 --save_steps 2500 --save_total_limit 40 --evaluation_strategy steps --eval_dataset_size 1024  --max_eval_samples 1000 --per_device_eval_batch_size 1 --max_new_tokens 32 --dataloader_num_workers 1 --group_by_length --logging_strategy steps --remove_unused_columns False --do_train --do_eval --do_mmlu_eval --lora_r 128 --lora_alpha 16 --lora_modules all --double_quant --quant_type nf4 --bf16 --bits 16 --warmup_ratio 0.03 --lr_scheduler_type constant --gradient_checkpointing --dataset alpaca --source_max_len 384 --target_max_len 128  --per_device_train_batch_size 1 --gradient_accumulation_steps 16 --max_steps 5000 --eval_steps 2500 --learning_rate 0.00005 --adam_beta2 0.999  --max_grad_norm 0.3 --lora_dropout 0.1 --weight_decay 0.0  --seed 2 --cache_dir /mnt/t-qingru/cache/lora --mmlu_split test &
    - sleep 3600
    - CUDA_VISIBLE_DEVICES=3 torchrun --rdzv-backend=c10d --rdzv-endpoint=localhost:40 --nnodes=1 --nproc-per-node=1 qlora_lora.py --ddp_find_unused_parameters False --model_name_or_path meta-llama/Llama-2-7b-hf --use_auth --output_dir /mnt/t-qingru/exp_results/lora_alpaca/rank128/5e-5/seed3 --logging_steps 10  --save_strategy steps --data_seed 45 --save_steps 2500 --save_total_limit 40 --evaluation_strategy steps --eval_dataset_size 1024  --max_eval_samples 1000 --per_device_eval_batch_size 1 --max_new_tokens 32 --dataloader_num_workers 1 --group_by_length --logging_strategy steps --remove_unused_columns False --do_train --do_eval --do_mmlu_eval --lora_r 128 --lora_alpha 16 --lora_modules all --double_quant --quant_type nf4 --bf16 --bits 16 --warmup_ratio 0.03 --lr_scheduler_type constant --gradient_checkpointing --dataset alpaca --source_max_len 384 --target_max_len 128  --per_device_train_batch_size 1 --gradient_accumulation_steps 16 --max_steps 5000 --eval_steps 2500 --learning_rate 0.00005 --adam_beta2 0.999  --max_grad_norm 0.3 --lora_dropout 0.1 --weight_decay 0.0  --seed 3 --cache_dir /mnt/t-qingru/cache/lora --mmlu_split test
